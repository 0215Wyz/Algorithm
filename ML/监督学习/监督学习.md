**每当想要根据给定输入预测某个结果，并且还有输入/输出对的示例时，都应该使用`监督学习`,输入/输出对构成了训练集，用来构建机器学习模型，目标是对从未见过的新数据做出准确预测。监督学习通常需要人力来构建训练集。**
<br/><br/>
# **分类与回归**
- `分类`与`回归`是监督机器学习的主要两种。

- 分类问题

    - 目标：预测`类别标签`

    - 二分类（在两个类别之间进行区分的一种特殊情况）

        - 将电子邮件分为垃圾邮件和非垃圾邮件就是一个二分类的实例

        - 正类，反类。这里的“正”不代表好的那一方面，或者是正数，而是代表`研究对象`，“正”可能指的是垃圾邮件这一类别
    
    - 多分类（在两个以上的类别之间进行区分）...

        - 鸢尾花

        - 根据网站上的文本预测网站所用的语言

- 回归任务

    - 目标：预测一个连续值（编程术语：浮点数；数学术语：实数）

    - 根据教育水平、年龄和居住地来预测一个人的年收入

    - 根据上一年的产量、天气和农场员工数等属性来预测玉米农场的产量

- 两问题区别在于，输出是否具有某种`连续性`

# **泛化、过拟合、欠拟合**

- `泛化`：一个模型能够对没见过的数据进行准确预测，就说它能够从训练<u>**泛化**</u>到测试集，要构建一个泛化精度尽可能高的模型

- 判断一个算法在新数据上表现好坏的唯一度量就是在测试集上进行评估

- 简单模型对新数据的泛化能力更好

- `过拟合`：构建一个对现有数据量来说过于复杂的模型。

    - 在拟合模型时过分关注训练集的细节，得到一个在训练集上表现很好、但不能泛化到新数据的模型，那么就存在过拟合

- `欠拟合`：选择**过于**简单的模型

> 模型越复杂，在训练数据上的预测数据就越好。但是，如果模型过于复杂，就会过度关注训练集中每个单独的数据点，模型就不能很好的泛化到新数据上。二者之间存在一个最佳的位置，可以得到最好的泛化性能。

- 更大的数据集可以用来构建更复杂的模型。收集更多的数据，是当构建更复杂的模型，对监督学习任务往往更加有用。

# **监督学习算法**
- 一些样本数据集

    - 模拟的二分类数据集示例：`forge数据集`

    - 模拟的`wave数据集`来说明回归算法，wave数据集只有一个输入特征和一个连续的目标变量（或**响应**）后者是模型想要预测的对象

    - 从特征较少的数据集（**低维**数据集）中得出的结论可能并不适合于特征较多的数据集（**高维**数据集）

    - `威斯康星州乳腺癌数据集`（简称cancer），每个肿瘤都被标记为“良性”（benign）或“恶性”（malignant），其任务是基于人体组织的测量数据来学习预测肿瘤是否为恶性

    - `波士顿房价数据集`。利用犯罪率、是否临近查尔斯河、公路可达性等信息，来预测20世纪70年代波士顿地区房屋价格的中位数
 
 - k近邻

    - k-NN算法可以说是最简单的机器学习算法。构建模型是需要保存训练数据集即可。想要对新数据点做出预测，算法会在训练数据集中找到最近的数据点，也就是它的“最近邻”。

        - 1.k近邻分类

            - k-NN算法最简单的版本就是只考虑一个最近邻。预测结果就是这个训练数据集的已知输出。

            - 对于多分类的问题数一数每个类别分别有多少个邻居，然后将最常见的类别作为预测结果。
        
        - 2.分析KNeighborsClassifier

            - 对于二位数据集，可以画出所有可能的测试点的预测结果。根据平面中每个点所属的类别对平面进行着色。这样可以查看`决策边界`。

            - 使用更少的邻居对应更高的模型复杂度，使用更多的邻居对应更低的模型复杂度。

        - 3.k近邻回归

        - 4.分析KNeighborsRegressor

            - 对于我们的一维数据集，可以查看所有特征值所对应的预测结果。

            ![不同n_neighbors值的k近邻回归的预测结果对比](https://raw.githubusercontent.com/ADiscipleofSherlockHolmes/Algorithm/master/ML/picture/%E4%B8%8D%E5%90%8Cn_neighbors%E5%80%BC%E7%9A%84k%E8%BF%91%E9%82%BB%E5%9B%9E%E5%BD%92%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%AF%B9%E6%AF%94.PNG)

            - 从图中可以看出，仅使用单一邻居的时候，训练集中的每一个点都对预测结果有显著影响，预测结果的图像都经过所有的数据点。这导致预测结果非常不稳定。更多的邻居之后，预测结果变得更加平滑，但对训练数据的拟合不好。

        - 5.优点，缺点和参数

            - KNeighbors分类器有两个重要的参数：邻居个数与数据点之间距离的度量方法。在实践中，使用较小的邻居个数往往可以得到比较好的结果。

            - k-NN的优点之一就是模型容易理解；构建最近邻模型的速度通常很快，如果训练集很大，预测速度可能会比较慢。

            - 使用k-NN算法时，对于有`很多特征`的数据集往往效果不好，对于大多数特征的大多数取值都为0的数据集（`稀疏数据集`）来说，这一算法效果尤其不好。实践中往往不会用到。
```python
train_test_split()
KNeighborsClassifier()
KNeighborsRegressor()
fit
score
```

- 线性模型

    - 线性模型利用输入特征的**线性函数**来进行预测。

    - 有许多不同的线性回归模型，这些模型之间的区别在于**如何从训练数据中学习参数w和b**，以及**如何控制模型复杂度**

    - 1.用于回归的线性模型

        - 对于更多特征的数据集，w包含沿每个特征坐标轴的斜率。或者，也可以将预测的响应值看作输入特征的加权求和，权重由w元素给出（可以取负值）

        - 用于回归的线性模型可以表示为这样的回归模型：对单一特征的预测结果是一条直线，两个特征时是一个平面，或者在更高维度（即更多特征）时是一个超平面

        - 直线的预测能力非常受限，似乎数据的所有细节都丢失了。

        - 对于多个特征的数据集而言，线性模型可以非常强大。特别地，<u>如果特征数量大于训练数据点的数量，任何目标y都可以（在训练集上）用线性函数完美拟合。 </u>(???)

        ![](https://raw.githubusercontent.com/ADiscipleofSherlockHolmes/Algorithm/master/ML/picture/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%AF%B9wave%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.PNG)



    - 2.线性回归（普通最小二乘法/Ordinary Least Squares（OLS））

        - 是回归问题最简单也最经典的线性方法。

        - 线性回归寻找参数w和b，使得对训练集的预测值与真实的回归目标值y之间的**均方误差**最小。

            - 均方误差：预测值与真实值之差的平方和除以样本数。

        - 线性回归没有参数，所以无法控制模型的复杂度。

        - 对于一维数据来说，过拟合的风险很小，因为模型非常简单（或受限）。然而，对于更高维的数据（即有大量特征的数据集）线性模型将会变得更加强大，过拟合的可能性也会变大。

    - 3.岭回归

        - 岭回归也是一种用于回归的线性模型，它的预测公式与普通最小二乘法相同。

        - 岭回归中，对系数（w）的选择不仅要在训练数据上得到好的预测结果，而且还要**拟合附加约束**。

        - 系数尽量小。（w所有的所有元素都应该接近于0），意味着每个特征对输出的影响尽量小（即斜率很小），同时仍给出很好的预测结果。

        - 这种**约束**是所谓的**正则化**。
        
            - 正则化是指对模型做显式约束，以避免过拟合。岭回归用到的被称为**L2正则化**。

        - 由下图，无论是岭回归还是线性回归，所有数据集大小对应的训练分数都要高于测试分数。由于岭回归是正则化的，所以它的训练分数要低于线性回归的训练分数。低于400个数据点，线性回归学习不到任何内容。如果有足够多的训练数据，正则化变得不那么重要，并且岭回归和线性回归将具有相同的性能。

        ![岭回归和线性回归在波士顿房价数据集上的学习曲线](https://raw.githubusercontent.com/ADiscipleofSherlockHolmes/Algorithm/master/ML/picture/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9C%A8%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.PNG)
    
    - 4.Lasso

        - 使用Lasso也是约束系数使其接近0，用到的方法是**L1正则化**。

            - L1正则化的结果是，使用lasso时某些系数**刚好为0**。这说明某些特征被系数完全忽略。看作是一种自动化的特征选择。这样使模型更容易理解，也可以呈现模型最重要的特征。

            - 在两个模型中，一般首选岭回归，如果特征很多，你认为只有其中几个是重要的，就选择Lasso。Lasso可以给出更容易解释的模型，因为它只选择了一部分输入特征。

    - 5.用于分类的线性模型

        - 线性模型也广泛应用于分类问题。

        - 如果函数值小于0，我们就预测类别-1；如果函数值大于0，我们就预测类别+1。对于所有用于分类的线性模型，这个预测规则都是通用的。

        - 对于用于分类的线性模型，**决策边界**是输入的线性函数。线性分类器是利用直线，平面，超平面来分开两个类别的分类器。

        - 学习线性模型有很多算法，区别在于：  1.系数和截距的特定组合对训练数据拟合好坏的度量方法 （**损失函数**）  2.是否使用正则化方法，以及使用哪种正则化  

        -  最常见的两种线性分类算法：
            
            - Logistic回归，在linear_model.LogisticRegression中实现，**虽然名字中有“回归”二字，但其实是分类算法**

            - 线性支持向量机（SVM），在svm.LinearSVC（SVC代表支持向量分类器）中实现

            ![线性SVM和Logistic回归在forge数据集上的决策边界](https://raw.githubusercontent.com/ADiscipleofSherlockHolmes/Algorithm/master/ML/picture/%E7%BA%BF%E6%80%A7SVM%E5%92%8CLogistic%E5%9B%9E%E5%BD%92%E5%9C%A8forge%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.PNG)

            - 两个模型都默认使用了L2正则化，就像Ridge对回归所做的那样。对于LinearSVC，LogisticRegression，决定正则化强度的权衡参数叫作C。C值越大，正则化越弱，尽可能的将训练集拟合到最好，如果C值越小，模型更强调使系数向量（w）趋近于0。

            - 较小的C让算法尽量适应“大多数”数据点，而较大的C更强调每个数据点**都**分类正确的重要性。

            ![不同C值的线性SVM在forge数据集上的决策边界](https://raw.githubusercontent.com/ADiscipleofSherlockHolmes/Algorithm/master/ML/picture/%E4%B8%8D%E5%90%8CC%E5%80%BC%E7%9A%84%E7%BA%BF%E6%80%A7SVM%E5%9C%A8forge%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C.PNG)

            - 左侧的图，C值很小，对应正则化很强。强正则化的模型会对应一条相对水平的线；中间的图，C值稍大，模型更关注两个分类错误的样本，使决策边界的斜率变大；右侧的图，模型C的值非常大，使得决策边界的斜率也很大，模型很可能过拟合。

    
    - 6.用于多分类的线性模型

- 朴素贝叶斯分类器

- 决策树

- 决策树集成

- 核支持向量机

- 神经网络

# **分类器的不确定度估计**

- 决策函数

- 预测概率

- 多分类问题的不确定度






















        


    